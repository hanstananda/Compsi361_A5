{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from numba import njit, jit\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from functools import reduce\n",
    "from typing import List, Set, Dict\n",
    "import math\n",
    "from math import log\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    dataframe = {}\n",
    "    with open(filename, \"r\" , newline='') as csvfile:\n",
    "        DictReader = csv.DictReader(csvfile)\n",
    "        init = False\n",
    "        for row in DictReader:\n",
    "            for key,value in row.items():\n",
    "                if not init:\n",
    "                    dataframe[key] = [value]\n",
    "                else:\n",
    "                    dataframe[key].append(value)\n",
    "            init = True\n",
    "        \n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_base_df = read_csv(\"trg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'A', 'E', 'E', 'B']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_base_df[\"class\"][:5] # Sanity check for data to be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidation():\n",
    "    \"\"\"\n",
    "    Cross Validation class. Custom-made for easier dataset management and testing. \n",
    "    \n",
    "    Parameters: \n",
    "        dataframe(DataFrame/ Dictionary of list from read_csv() function): Data to be used in cross-validation. \n",
    "        num_split(Int): The number of split to be executed. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, num_split=10):\n",
    "        self.df = dataframe\n",
    "        self.split = num_split\n",
    "        self.df_length = 0\n",
    "        self.dfs = []\n",
    "        for i in range(self.split):\n",
    "            self.dfs.append(dict())\n",
    "        self._shuffle_and_split()\n",
    "        \n",
    "    \n",
    "    def _shuffle_and_split(self, random_state=42): \n",
    "        \"\"\"\n",
    "        Internal function. Called when CrossValidation is initialized. \n",
    "        Shuffle the given data and split them according to the number of split \n",
    "        \n",
    "        Parameters:\n",
    "            random_state(Int) : The random seed to be used to shuffle the data\n",
    "        \"\"\"\n",
    "        self.df_length = len(self.df[\"abstract\"])\n",
    "        shuffle_index = list(range(self.df_length)) \n",
    "        random.Random(random_state).shuffle(shuffle_index)# Shuffle the indexes \n",
    "        for i in self.df.keys():\n",
    "            shuffled_df = []\n",
    "            for j in range(self.df_length):\n",
    "                shuffled_df.append(self.df[i][j])\n",
    "            split_chunks = np.array_split(shuffled_df, self.split)\n",
    "            for j in range(len(split_chunks)):\n",
    "                self.dfs[j][i] = np.asarray(split_chunks[j])\n",
    "        \n",
    "    def get_ith_cv(self,i):\n",
    "        \"\"\"\n",
    "        Get the training set and test set k-th fold. \n",
    "        \n",
    "        Parameters: \n",
    "            i(Int): Used to choose which fold to be used\n",
    "        \"\"\"\n",
    "        i%=self.split\n",
    "        train = dict()\n",
    "        test = self.dfs[i]\n",
    "        for j in self.dfs[0].keys():\n",
    "            train[j] = []\n",
    "        for x in range(self.split):\n",
    "            if x == i:\n",
    "                continue\n",
    "            for j in self.dfs[x].keys():\n",
    "                train[j].append(self.dfs[x][j])\n",
    "        for j in train.keys():\n",
    "            train[j] = np.concatenate(train[j])\n",
    "        return train,test\n",
    "    \n",
    "    def train_and_validate(self): \n",
    "        \"\"\"\n",
    "        Start the k-fold validation test\n",
    "        \"\"\"\n",
    "        \n",
    "        accuracy = np.zeros(self.split, dtype=float)\n",
    "        for i in range(self.split):\n",
    "            print(\"Fold \",i+1)\n",
    "            train_df, test_df = self.get_ith_cv(i)\n",
    "            classifier = MultinomialNBC()\n",
    "            classifier.add_ignored_words(stopwords)\n",
    "            classifier.train(train_df)\n",
    "            predictions = classifier.predict(test_df)\n",
    "            Y_test = test_df[\"class\"]\n",
    "            accuracy[i] = get_accuracy(Y_test, predictions)\n",
    "            print(accuracy[i])\n",
    "            \n",
    "        return accuracy\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_data = CrossValidation(training_base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open(\"english_stopwords.txt\", \"r\") as f:\n",
    "    stopwords = [i.strip() for i in f.readlines()] # Read the file, strip newlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultinomialNBC():\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes Classifier that takes word counts from abstracts as its features. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.class_mapping = dict()\n",
    "        self.reverse_class_mapping = dict()\n",
    "        self.word_mapping = dict()\n",
    "        self.reverse_word_mapping = dict()\n",
    "        self.word_counts = []\n",
    "        self.probs = []\n",
    "        self.words = []\n",
    "        self.num_unique_words = 0\n",
    "        self.alpha_i = 1\n",
    "        self.ignored_words = set()\n",
    "    \n",
    "    def add_ignored_words(self, words): # \n",
    "        \"\"\"\n",
    "        Add words to be ignored by the classifier\n",
    "        \n",
    "        Parameters:\n",
    "            words(List[String]): List of words to be ignored\n",
    "            \n",
    "        \"\"\"\n",
    "        for word in words:\n",
    "            self.ignored_words.add(word)\n",
    "    \n",
    "    def train(self, train_data): \n",
    "        \"\"\"\n",
    "        Train the classifier from the given training data\n",
    "        \n",
    "        Parameters:\n",
    "            train_data(DataFrame/ Dictionary of list from read_csv() function): Training data for the classifier\n",
    "            \n",
    "        \"\"\"\n",
    "        y = train_data[\"class\"]\n",
    "        x = train_data[\"abstract\"]\n",
    "        num_training_data = len(x)\n",
    "        \n",
    "        self.classes , self.classes_count = np.unique(y, return_counts=True)\n",
    "        self.num_class = len(self.classes)\n",
    "        print(\"number of classes found in train data:\",self.num_class)\n",
    "        print(\"classes: \", self.classes)\n",
    "        for i in range(self.num_class):\n",
    "            self.class_mapping[self.classes[i]]=i\n",
    "            self.reverse_class_mapping[i] = self.classes[i]\n",
    "        \n",
    "        print(\"Finished preprocess classes: \", time.perf_counter())\n",
    "        self._get_overall_word_count(x)\n",
    "        print(\"Finished count in overall words: \",time.perf_counter())\n",
    "        \n",
    "        print(\"number of unique words=\",self.num_unique_words)\n",
    "        # print(self.word_counts)\n",
    "        # self.word_counts_per_category = np.zeros([self.num_class, self.num_unique_words], dtype=int) \n",
    "        self.weighted_words= np.zeros([len(x), self.num_unique_words], dtype= float)\n",
    "        self.words_occurence= np.zeros(self.num_unique_words, dtype= int)\n",
    "        \n",
    "        for i in range(num_training_data):\n",
    "            words, counts = np.unique(self._get_sanitized_wordlist(x[i]), return_counts=True)\n",
    "#             downweight_ratio = math.sqrt(sum(counts**2))\n",
    "            \n",
    "            for j in range(len(words)):\n",
    "                if words[j] not in self.word_mapping:\n",
    "                    continue\n",
    "                word_index = self.word_mapping[words[j]] \n",
    "                self.weighted_words[i][word_index]= counts[j]\n",
    "                self.words_occurence[word_index]+=1\n",
    "#                 self.weighted_words[i][word_index] /= downweight_ratio\n",
    "\n",
    "        print(\"Finished counting words per category at\", time.perf_counter())\n",
    "\n",
    "        self._downweight_common_words(num_training_data)\n",
    "                            \n",
    "        print(\"Finished downweighitng common words at\", time.perf_counter())\n",
    "        \n",
    "        self.weighted_words_per_class = np.zeros([self.num_class, self.num_unique_words], dtype=float)\n",
    "        self.weighted_words_class = np.zeros(self.num_class, dtype=float)\n",
    "        \n",
    "        for i in range(num_training_data):\n",
    "            words, counts = np.unique(self._get_sanitized_wordlist(x[i]), return_counts=True)\n",
    "            for j in range(len(words)):\n",
    "                if words[j] not in self.word_mapping:\n",
    "                    continue\n",
    "                data_class_index = self.class_mapping[y[i]]\n",
    "                word_index = self.word_mapping[words[j]] \n",
    "                self.weighted_words_per_class[data_class_index][word_index]+= self.weighted_words[i][word_index]\n",
    "        for i in range(self.num_class):\n",
    "            self.weighted_words_class[i] = sum(self.weighted_words_per_class[data_class_index])\n",
    "            \n",
    "                \n",
    "        self.probs = np.zeros([self.num_class, self.num_unique_words+1], dtype=float)\n",
    "        \n",
    "        print(\"Finished calculating weighted words: \",time.perf_counter())\n",
    "        \n",
    "        \n",
    "        for i in range(self.num_class):\n",
    "            for j in range(self.num_unique_words):\n",
    "                self.probs[i][j] = math.log(self.weighted_words_per_class[i][j] + 1) - \\\n",
    "                    math.log(self.weighted_words_class[i] + self.num_unique_words)\n",
    "                \n",
    "        print(\"Training finished at \", time.perf_counter())\n",
    "        \n",
    "    def _downweight_common_words(self, num_training_data:int):\n",
    "        \"\"\"\n",
    "        Internal function. Feature enhancement to the weighted words to downweight common words. \n",
    "        Implementation based on the given tutorial slides\n",
    "        \n",
    "        Parameters: \n",
    "            num_training_data(Int): Length of training data to be processed\n",
    "        \"\"\"\n",
    "        for i in range(num_training_data):\n",
    "            for j in range(self.num_unique_words):\n",
    "                self.weighted_words[i][j] *= log(num_training_data/ self.words_occurence[j])\n",
    "    \n",
    "    def _get_sanitized_wordlist(self, sentence):\n",
    "        \"\"\"\n",
    "        Internal function. Get the sanitized list of words from the given sentence\n",
    "        \n",
    "        Parameters: \n",
    "            sentence(String): Sentence to be processed\n",
    "        \"\"\"\n",
    "        wordlist = [x.strip().lower() for x in sentence.split(' ')] # split by space then make it lowercase\n",
    "        for i in range(len(wordlist)):\n",
    "            wordlist[i] = re.sub('[^a-z]+', '', wordlist[i]) # Remove everything that is not related to alphabet\n",
    "        return wordlist\n",
    "    \n",
    "    def _construct_word_dictionary(self, sentence, word_dict= {}):\n",
    "        wordlist = self._get_sanitized_wordlist(sentence)\n",
    "        for word in wordlist:\n",
    "            if word in self.ignored_words:\n",
    "                continue\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "        return word_dict\n",
    "        \n",
    "    \n",
    "    def _get_overall_word_count(self, data):\n",
    "        \"\"\"\n",
    "        Internal function. Get the overall word count from the given data \n",
    "        \n",
    "        Parameters: \n",
    "            data(List[String]): List of abstracts which word counts need to be analyzed\n",
    "        \"\"\"\n",
    "        word_dict = dict()\n",
    "        \n",
    "        for abstract in data:\n",
    "            word_dict = self._construct_word_dictionary(abstract, word_dict= word_dict)\n",
    "#         print(word_dict)\n",
    "            \n",
    "        self.word_counts = np.zeros(len(word_dict), dtype =int)\n",
    "        self.words = word_dict.keys()\n",
    "        \n",
    "        for key,value in word_dict.items():\n",
    "            self.word_mapping[key] = self.num_unique_words\n",
    "            self.reverse_word_mapping[self.num_unique_words] = key\n",
    "            self.word_counts[self.num_unique_words] = value\n",
    "            self.num_unique_words+=1\n",
    "            \n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"\n",
    "        Predict the given test data. \n",
    "        Note: Classifier have to be trained first in order to return meaningful results! \n",
    "        \n",
    "        Parameters:\n",
    "            test_data(DataFrame/ Dictionary of list from read_csv() function): Test data to be predicted by the classifier\n",
    "            \n",
    "        \"\"\"\n",
    "        x = test_data[\"abstract\"]\n",
    "        prediction_probs = np.zeros([len(x), self.num_class], dtype=float)\n",
    "        for i in range(len(x)):\n",
    "            words, counts = np.unique(self._get_sanitized_wordlist(x[i]), return_counts=True)\n",
    "            sigma_fi_fact = log(math.factorial(sum(counts)))\n",
    "            pi_fi_fact = 0\n",
    "            for k in range(len(words)): \n",
    "                pi_fi_fact+= log(math.factorial(counts[k]))\n",
    "            \n",
    "            for j in range(self.num_class):\n",
    "                prediction_probs[i][j] = sigma_fi_fact - pi_fi_fact\n",
    "                for k in range(len(words)):\n",
    "                    if words[k] not in self.word_mapping:\n",
    "                        continue\n",
    "                    word_index = self.word_mapping[words[k]]\n",
    "                    prediction_probs[i][j]+= self.probs[j][word_index] * counts[k]\n",
    "                    \n",
    "        predictions = [''] * len(x)\n",
    "        for i in range(len(x)):\n",
    "            prediction_i = np.argmax(prediction_probs[i], axis=0)\n",
    "            predictions[i] = self.reverse_class_mapping[prediction_i]\n",
    "        return predictions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 400\n",
      "1 B the 4 202 353 bp genome of the alkaliphilic bacterium bacillus halodurans c-125 contains 4066 predic\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = crossval_data.get_ith_cv(1)\n",
    "print(len(train_df[\"abstract\"]), len(test_df[\"abstract\"]))\n",
    "print(train_df[\"id\"][0], train_df[\"class\"][0], train_df[\"abstract\"][0][:100]) # Sanity check whether shuffle is performed successfuly and consistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNBC()\n",
    "classifier.add_ignored_words(stopwords)\n",
    "print(len(classifier.ignored_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1159.5957824\n",
      "Finished count in overall words:  1160.4560121\n",
      "number of unique words= 23510\n",
      "Finished counting words per category at 1162.0920339\n",
      "Finished downweighitng common words at 1243.4408652\n",
      "Finished calculating weighted words:  1245.1020893\n",
      "Training finished at  1245.2496632\n"
     ]
    }
   ],
   "source": [
    "probs = classifier.train(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_df)\n",
    "Y_test = test_df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(test, predictions):\n",
    "    correct = 0 \n",
    "    false = 0 \n",
    "    for i in range(len(test)):\n",
    "        if test[i]==predictions[i]:\n",
    "            correct+=1\n",
    "        else:\n",
    "            false+=1\n",
    "    return correct/(correct+false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1283.9361662\n",
      "Finished count in overall words:  1284.8006219\n",
      "number of unique words= 23529\n",
      "Finished counting words per category at 1286.4248422\n",
      "Finished downweighitng common words at 1367.9061821\n",
      "Finished calculating weighted words:  1369.5611215\n",
      "Training finished at  1369.712379\n",
      "0.8875\n",
      "Fold  2\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1370.1640729\n",
      "Finished count in overall words:  1371.0326773\n",
      "number of unique words= 23510\n",
      "Finished counting words per category at 1372.6425347\n",
      "Finished downweighitng common words at 1454.2144242\n",
      "Finished calculating weighted words:  1455.8921428\n",
      "Training finished at  1456.0408636\n",
      "0.855\n",
      "Fold  3\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1456.4927917\n",
      "Finished count in overall words:  1457.3636551\n",
      "number of unique words= 23534\n",
      "Finished counting words per category at 1458.9847748\n",
      "Finished downweighitng common words at 1540.6476547\n",
      "Finished calculating weighted words:  1542.2828725\n",
      "Training finished at  1542.4335493\n",
      "0.91\n",
      "Fold  4\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1542.8871277\n",
      "Finished count in overall words:  1543.7676455\n",
      "number of unique words= 23589\n",
      "Finished counting words per category at 1545.4144529\n",
      "Finished downweighitng common words at 1624.8092974\n",
      "Finished calculating weighted words:  1626.4620769\n",
      "Training finished at  1626.6109644\n",
      "0.86\n",
      "Fold  5\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1627.0622981\n",
      "Finished count in overall words:  1627.9384347\n",
      "number of unique words= 23618\n",
      "Finished counting words per category at 1629.564485\n",
      "Finished downweighitng common words at 1711.613108\n",
      "Finished calculating weighted words:  1713.26297\n",
      "Training finished at  1713.411228\n",
      "0.87\n",
      "Fold  6\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1713.8563912\n",
      "Finished count in overall words:  1714.7292227\n",
      "number of unique words= 23831\n",
      "Finished counting words per category at 1716.3751536\n",
      "Finished downweighitng common words at 1797.0931098\n",
      "Finished calculating weighted words:  1798.7619407\n",
      "Training finished at  1798.9133259\n",
      "0.91\n",
      "Fold  7\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1799.3623985\n",
      "Finished count in overall words:  1800.231938\n",
      "number of unique words= 23787\n",
      "Finished counting words per category at 1801.8602384\n",
      "Finished downweighitng common words at 1885.7774547\n",
      "Finished calculating weighted words:  1887.453938\n",
      "Training finished at  1887.6041047\n",
      "0.885\n",
      "Fold  8\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1888.0469429\n",
      "Finished count in overall words:  1888.9178632\n",
      "number of unique words= 23775\n",
      "Finished counting words per category at 1890.5542292\n",
      "Finished downweighitng common words at 1970.4522562\n",
      "Finished calculating weighted words:  1972.1172056\n",
      "Training finished at  1972.268356\n",
      "0.8875\n",
      "Fold  9\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  1972.7193542\n",
      "Finished count in overall words:  1973.5955062\n",
      "number of unique words= 23717\n",
      "Finished counting words per category at 1975.2167645\n",
      "Finished downweighitng common words at 2055.1546266\n",
      "Finished calculating weighted words:  2056.8380501\n",
      "Training finished at  2056.9895649\n",
      "0.9025\n",
      "Fold  10\n",
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  2057.4518938\n",
      "Finished count in overall words:  2058.3460852\n",
      "number of unique words= 23652\n",
      "Finished counting words per category at 2060.0144425\n",
      "Finished downweighitng common words at 2145.8214569\n",
      "Finished calculating weighted words:  2147.5239213\n",
      "Training finished at  2147.6733807\n",
      "0.8875\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = crossval_data.train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8875, 0.855 , 0.91  , 0.86  , 0.87  , 0.91  , 0.885 , 0.8875,\n",
       "       0.9025, 0.8875])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854999999999998"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(overall_accuracy)/len(overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes found in train data: 4\n",
      "classes:  ['A' 'B' 'E' 'V']\n",
      "Finished preprocess classes:  55.1163301\n",
      "Finished count in overall words:  56.0946357\n",
      "number of unique words= 25069\n",
      "Finished counting words per category at 58.012002\n",
      "Finished downweighitng common words at 58.012507\n",
      "Finished calculating weighted words:  59.8978714\n",
      "Training finished at  60.0563226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -5.159973  ,  -5.79223181,  -8.73456331, ..., -12.64658631,\n",
       "        -12.64658631,   0.        ],\n",
       "       [ -2.84038088,  -3.81443641,  -6.11425202, ..., -12.64658631,\n",
       "        -12.64658631,   0.        ],\n",
       "       [ -2.52818901,  -3.52814196,  -6.48960733, ..., -11.95343913,\n",
       "        -11.95343913,   0.        ],\n",
       "       [ -5.15885255,  -6.02784733,  -9.65085404, ..., -12.64658631,\n",
       "        -12.64658631,   0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNBC()\n",
    "classifier.train(training_base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check \n",
    "predictions = classifier.predict(test_df)\n",
    "Y_test = test_df[\"class\"]\n",
    "get_accuracy(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base_df = read_csv(\"tst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'B',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_base_predictions = classifier.predict(test_base_df)\n",
    "test_base_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers successfully writen to answers3.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'answers3.1.csv'\n",
    "\n",
    "with open(filename, 'w+', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'class']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for i in range(len(test_base_predictions)):\n",
    "        writer.writerow({\n",
    "            'id': test_base_df['id'][i],\n",
    "            'class': test_base_predictions[i]\n",
    "        })\n",
    "    print(\"Answers successfully writen to\",filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5'] ['B', 'E', 'E', 'E', 'E']\n"
     ]
    }
   ],
   "source": [
    "check_answer = read_csv(filename)\n",
    "print(check_answer[\"id\"][:10], check_answer[\"class\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}